## 开发环境配置

## 一、win10安装

### 1、安装jdk

jdk-8u241-windows-x64 版本及其以上

环境变量设置：

```
JAVA_HOME
Path
CLASSPATH：xxx\java\lib
```

注意：java安装路径不能有空格。

测试安装成功
`java -version`

### 2、安装scala

下载并解压，配置环境变量

```
SCALA_HOME = D:\bigdata\scala\scala-2.11.12
Path：  %SCALA_HOME%\bin;
```



### 3、安装spark和hadoop

下载 `spark-2.4.7-bin-hadoop2.7.tgz`

下载地址：https://archive.apache.org/dist/spark/spark-2.4.7/

下载 hadoop2.7x

然后解压、配置环境变量

```
SPARK_HOME =  D:\software\spark-2.4.7-bin-hadoop2.7
```

```
SPARK_HOME = D:\software\spark-2.4.7-bin-hadoop2.7
Path  %SPARK_HOME%\bin
HADOOP_HOME = D:\software\hadoop-2.7.1
Path  %HADOOP_HOME%\bin
```

### 4 下载winutil.exe

将winutils.exe文件放到Hadoop的bin目录下（C:\SoftWare\hadoop-2.7.1\bin），然后以管理员的身份打开cmd，然后通过cd命令进入到Hadoop的bin目录下，然后执行以下命令：

winutils.exe chmod 777 c:\tmp\Hive



### 5 、pyspark相关修改：

将`C:\SoftWare\spark-2.4.7-bin-hadoop2.7\python\lib`中的
`pyspark`和`py4j-0.10.9-src`复制到
`C:\Users\18177\Anaconda3\Lib\site-packages`文件夹下，
同时，删除该文件夹下的pyspark 和 py4j文件夹。
解压这两个压缩文件，得到新的pyspark 和 py4j文件夹。解压完成，删除两个压缩文件。


### 5. 2 pycharm使用pyspark

安装完成，如果在cmd里输入`pyspark`成功，但是pycharm运行失败，把`JAVA_HOME,SPARK_HOME`添加到py文件的环境变量里。或者重启之后，pycharm会加载最新版的系统环境变量。



## 二、IDEA设置

配置前进后台快捷按钮 https://www.jianshu.com/p/dbe83ca9f79c

### 1、IDEA添加scala支持

1、settings-plugins 搜索scala并下载

2、main目录下新建一个scala文件夹。并设置为source root

3、project 根目录右键 ADD FRAMEWORK SUPPORT,勾选scala

4、配置scala SDK

### 2、maven依赖改为国内

project根目录，右键，maven 选择 “open settting.xml”

复制下面内容

```
<!--配置阿里云Maven镜像-->
     <mirror>
        <id>nexus-aliyun</id>
        <mirrorOf>central</mirrorOf>
        <name>Nexus aliyun</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public</url>
    </mirror>
     <!--配置华为云Maven镜像-->
    <mirror>
        <id>huaweicloud</id>
        <mirrorOf>*</mirrorOf>
        <url>https://mirrors.huaweicloud.com/repository/maven/</url>
    </mirror>
```



## 三、创建spark项目

### 1、new 一个maven项目，选择jdk1.8

### 2、补充groupId和artifactId

groupId和artifactId被统称为“坐标”是为了保证项目唯一性而提出的，如果你要把你项目弄到maven本地仓库去，你想要找到你的项目就必须根据这两个id去查找。

　groupId一般分为多个段，这里我只说两段，第一段为域，第二段为公司名称。域又分为org、com、cn等等许多，其中org为非营利组织，com为商业组织。举个apache公司的tomcat项目例子：这个项目的groupId是org.apache，它的域是org（因为tomcat是非营利项目），公司名称是apache，artigactId是tomcat。

　　比如我创建一个项目，我一般会将groupId设置为cn.zr，cn表示域为中国，zr是我个人姓名缩写，artifactId设置为testProj，表示你这个项目的名称是testProj，依照这个设置，你的包结构最好是cn.zr.testProj打头的，如果有个StudentDao，它的全路径就是cn.zr.testProj.dao.StudentDao

注：artifactId实际应该是项目名-模块名，不只是项目名

### 3、删除自带的src，创建一个module作为项目起点。

```
spark-core
```

右键spark-core  ADD FRAMEWORK SUPPORT,勾选scala

### 4、将依赖放到 spark-core目录下的 pom.xml中

```
    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>2.4.7</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-yarn_2.12</artifactId>
            <version>2.4.7</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>2.4.7</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.27</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.12</artifactId>
            <version>2.4.7</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
            <version>2.4.7</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/com.alibaba/druid -->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>druid</artifactId>
            <version>1.1.10</version>
        </dependency>
    </dependencies>
```

### 5、创建一个测试用例

#### 1、修改日志打印

resources 目录下 添加 log4j.properties，其内容

```
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd 
HH:mm:ss} %p %c{1}: %m%n
# Set the default spark-shell log level to ERROR. When running the spark-shell,
the
# log level for this class is used to overwrite the root logger's log level, so
that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=ERROR
# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=ERROR
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR
# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent
UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
```

#### 2、创建wordcount

新建package：core    再新建 package wc 。最后新建一个scala object。

```
package core.wc

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD

object Spark_01_WordCount {
  def main(args: Array[String]): Unit = {
    // Application
    // Spark框架
    // TODO 建立和Spark框架的连接
    // JDBC : Connection
    val sparConf = new SparkConf().setMaster("local").setAppName("WordCount")
    val sc = new SparkContext(sparConf)
    // 1、读取文件数据
    val fileRDD: RDD[String] = sc.textFile("D:\\bigdata\\spark\\spark-demo\\input\\word.txt")
    // 2、每一行数据进行拆分，形成一个一个单词
    val words = fileRDD.flatMap(_.split(" "))
    // 3. 将数据进行分组，便于统计 (word,iterator)
    val wordGroup = words.groupBy(word=>word)
    // 4 分组后的数据进行转换

    val wordToCount = wordGroup.map{
      case(word,list) =>{
        (word,list.size)
      }
    }
    // 5 打印
    val arr = wordToCount.collect()
    arr.foreach(println)


    // TODO 关闭连接
    sc.stop()
  }

}

```



## 四、linux安装

1、安装java

```
cd /opt/software/
tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
```

```
# 添加环境变量
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
# 使其生效
source /etc/profile
```

2、安装hadoop

```
cd /opt/software/
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
```

```
#HADOOP_HOME 配置环境变量
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
```



### 1、创建spark目录

```
mkdir /usr/spark
cd /usr/spark
```

### 2、下载spark安装包

```
https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz
```

### 3、解压并配置环境变量

```
tar -zxvf spark-2.4.2-bin-hadoop2.7.tgz
# 配置环境变量
vi /etc/profile
# 在profile文件中最后一行写入：
PATH=/usr/spark/spark-2.4.2-bin-hadoop2.7/bin:$PATH
# 然后使其生效
source /etc/profile
echo $PATH
```

### 4、安装pyspark模块

```
pip install pyspark
```

