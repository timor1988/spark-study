

## 一、架构

### 1、提交任务

```
spark-submit --master yarn \
--deploy-mode cluster \
--conf spark.pyspark.python=/usr/bin/python \
--py-files 7fresh-sco-fw_demo.zip \
pi.py 100
```

启动AM的运行命令是：

```
{{JAVA_HOME}}/bin/java -server -Xmx1024m -Djava.io.tmpdir={{PWD}}/tmp -Dspark.yarn.app.container.log.dir=<LOG_DIR> org.apache.spark.deploy.yarn.ApplicationMaster --class 'org.apache.spark.deploy.PythonRunner' --primary-py-file pi.py --arg '100' --properties-file {{PWD}}/__spark_conf__/__spark_conf__.properties 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
```

AM入口类是`org.apache.spark.deploy.yarn.ApplicationMaster`，

执行python脚本的入口类`--class`是`org.apache.spark.deploy.PythonRunner`。

### 2、PythonRunner启动Python过程

转载：https://blog.csdn.net/oTengYue/article/details/106376108



在 [Spark源码分析之ApplicationMaster运行流程](https://blog.csdn.net/oTengYue/article/details/105339515) 中我们介绍了，AM启动后最终会在 [org.apache.spark.deploy.yarn.ApplicationMaster](https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala) 类的`startUserApplication()`函数中反射实例化`--class`类用于初始化SparkContext，针对PySpark程序的运行，在此函数首先判断如果是py，则会重新组织参数，运行提交命令中的`org.apache.spark.deploy.PythonRunner`类（注：如果用户依赖的第三方|自定义的Python依赖包，则通过`launch_container.sh`设置PYTHONPATH的方式传递，因此第二个参数是空，如上文提交任务生成的格式，例如：`PythonRunner pi.py 空 100`）：

```
private def startUserApplication(): Thread = {
    logInfo("Starting the user application in a separate Thread")

    var userArgs = args.userArgs
    if (args.primaryPyFile != null && args.primaryPyFile.endsWith(".py")) {
      // When running pyspark, the app is run using PythonRunner. The second argument is the list
      // of files to add to PYTHONPATH, which Client.scala already handles, so it's empty.
      userArgs = Seq(args.primaryPyFile, "") ++ userArgs
    }
    ...
     mainMethod.invoke(null, userArgs.toArray)
```

[org.apache.spark.deploy.PythonRunner](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala) 类的main函数:

![](D:\学习\spark-study\image\PythonRunner.png)

开启两个东西：

1、py4j服务端： gatewayServer

2、python线程



#### 01. gatewayServer

![](D:\学习\spark-study\image\pyspark架构3.png)

如上图，在PythonRunner中初始化gatewayServer后启动一个守护线程用于启动服务，在守护线程中先创建一个Socket监听端口（传入的端口号为0，则系统会选择一个空闲端口进行监听），选择的端口会传递给Python进程来连接此gatewayServer，然后再新开启一个线程循环接受socket消息，此时服务端就启动完成了。当服务端监听到有socket连接来到时候，通过processSocket(socket)分发处理连接消息，流程如下：

![](D:\学习\spark-study\image\pyspark架构4.png)

从上图可以看出，processSocket()函数使用锁保证线程安全，分别对每个socket连接都创建GatewayConnection，在构造函数中重点关注从socket初始化了reader流和writer流，然后在GatewayConnection.startConnection()开辟一个线程处理消息，这样针对每个socket开辟一个线程的方式能够保证了processSocket()线程的并发性。在消息处理过程会根据消息指令选择不同的命令类处理消息，而commands（命令类Map）的构建同样是在创建GatewayConnection时候完成的，如下图：


我们继续看线程中具体的消息处理过程：

![](image\pyspark架构5.png)

如上图，在线程中会按行读取指令，根据指令选择对应的命令类进行后续处理（上图右侧是调用ConstructorCommand实例化类的示例），结果返回是在命令类中通过`write`函数以socket方式返回给Python客户端。

#### 02. Python客户端

PySpark中Py4j客户端的初始化是在SparkContext构造函数中完成的，初始化流程在launch_gateway

最终初始化关键代码在`_launch_gateway()`函数，获取gateway_port端口（运行在Yarn集群上是PythonRunner启动Py4j服务端后，将其写入到启动Python进程的环境变量中；如果直接本地启动PySpark则会调用命令拉起一个JVM启动Py4j服务端后获得端口），创建JavaGateway网关，然后调用java_import()函数把spark提供的api的package导入，最后赋值给SparkContext的_gateway变量（JavaGateway对象）和_jvm变量（JVMView对象），在PySpark中对JVM的调用实质都是通过`_jvm`变量来进行的，至此就完成了Py4j客户端的初始化。我们下面先看下如何使用_jvm创建SparkContext对象，如下：

```
    def _do_init(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer,
                 conf, jsc, profiler_cls):
        ...
        # Create the Java SparkContext through Py4J
        self._jsc = jsc or self._initialize_context(self._conf._jconf)
        # Reset the SparkConf to the one actually used by the SparkContext in JVM.
        self._conf = SparkConf(_jconf=self._jsc.sc().conf())
        ...
     def _initialize_context(self, jconf):
        """
        Initialize SparkContext in function to allow subclass specific initialization
        """
        return self._jvm.JavaSparkContext(jconf)   

```

上面介绍完了PySpark中Py4j客户端的初始化，下面我们看下客户端调用流程（由于下图已很清晰，不再分析源码，大家可以参照查阅源码）：

![](D:\学习\spark-study\image\pyspark架构6.png)

说明：

1、Python客户端的源码相对集中，以上流程分析 py4j/java_gateway.py 即可得出。
2、JavaObject是JVM中实例化的对象在Python中的影子（其target_id变量是JVM实例化后对象对象的Key值）。
3、JavaMember是对JavaObject中成员的封装，目前JavaMember只支持函数，不支持变量值（字段）。调用类变量值需创建JavaGateway时候设置gateway = JavaGateway(auto_field=True)（启用后JavaObject.__getattr__()函数会优先调用字段，可能会隐藏同名的类方法），PySpark默认未启用，因此PySpark不支持反射调用类变量。
4、Python通过调用__getattr__和__call__来实现对Java对象创建、属性访问和方法调用。



#### python和java的通信

为了不破坏 Spark 已有的运行时架构，Spark 在外围包装一层 Python API，借助 Py4j实现 Python 和 Java 的交互，进而实现通过 Python 编写 Spark 应用程序。其原理图下图：

<img src="image\pyspark架构1.png" style="zoom:50%;" />

Py4J提供了一套文本协议用来在tcp socket间传递命令，主要作用在Driver端（如上图左图范围），而在executor端则是通过启动的`pyspark.daemon`后通过socket直接通信的。

由上节我们知道`PythonRunner`在创建Python子进程时会把Py4J监听的端口写入到子进程的环境变量中，这样Python就知道从哪个端口访问JVM了，当然Python在创建JavaGateway时，也可以同时创建一个CallbackClient，实现JVM调用Python过程。默认情况下，PySpark Job是不会启动回调服务的，所有的交互都是 `Python -> JVM`模式，但在SparkStreaming中才会用到`JVM -> Python`的过程（本文不再重点讲解）。我们首先看一下Driver与Python的整体的运行流程图：

![](D:\学习\spark-study\image\pyspark架构2.png)

我们接上节，`PythonRunner`启动子Python进程运行 `python pi.py 100` 后，开始初始化SparkContext，如运行`pi.py`代码：

```
spark = SparkSession\
        .builder\
        .appName("PythonPi")\
        .getOrCreate()
```

首先会依次调用[session.py](https://github.com/apache/spark/blob/master/python/pyspark/sql/session.py) 和 [context.py](https://github.com/apache/spark/blob/master/python/pyspark/context.py) 进行初始化SparkContext，其代码调用链为 `SparkSession.Builder.getOrCreate() -> SparkContext.getOrCreate() -> SparkContext.__init__()`。

开始创建SparkContext

#### 1 getOrCreate方法

```
@classmethod
def getOrCreate(cls, conf=None):
    """
    Get or instantiate a SparkContext and register it as a singleton object.
    :param conf: SparkConf (optional)
    """
    with SparkContext._lock:
        if SparkContext._active_spark_context is None:
            SparkContext(conf=conf or SparkConf())
        return SparkContext._active_spark_context
```

这个是一个类方法，判断了类变量`_active_spark_context`是否为`None`，是的情况下，我们直接通过`SparkContext(conf=conf or SparkConf())`这样的方式实例化，返回的就是`_active_spark_context`，这个其实在后面可以看见，其实就是实例本身`self`。

#### 2 `__init__`方法

```
def __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None,
             environment=None, batchSize=0, serializer=PickleSerializer(), conf=None,
             gateway=None, jsc=None, profiler_cls=BasicProfiler):
    self._callsite = first_spark_call() or CallSite(None, None, None)
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
    try:
        self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,
                      conf, jsc, profiler_cls)
    except:
        # If an error occurs, clean up in order to allow future SparkContext creation:
        self.stop()
        raise
```

很清晰的看见，`_ensure_initialized`这个类函数被调用，里面的逻辑后面会描述，这里只需要知道，这个函数会给上文提到的类变量`_active_spark_context`进行赋值，一般情况下，是被赋值为了当前的实例。

#### 3 `_ensure_initialized`方法

```
SparkContext._gateway = gateway or launch_gateway(conf)
SparkContext._jvm = SparkContext._gateway.jvm
...
SparkContext._active_spark_context = instance
```

1. 首先对类变量`_gateway`和`_jvm`进行了初始化
2. 就是上面说的，对类变量`_active_spark_context`初始化为了当前实例

新启动一个Gateway赋值给 `_gateway`变量（JavaGateway对象）和 `_jvm`变量（JVMView对象），这样就可以通过这个`_jvm`变量来访问jvm中的Java对象和方法。下面我们分析 [java_gateway.py](https://github.com/apache/spark/blob/master/python/pyspark/java_gateway.py) 中 `launch_gateway()` 函数，代码如下：

#### 4 launch_gateway

大体上讲，就是初始化了python和jvm之间的通道，这个函数前面大体是在配置环境

```
gateway_port = int(os.environ["PYSPARK_GATEWAY_PORT"])

gateway = JavaGateway(
    gateway_parameters=GatewayParameters(port=gateway_port, auth_token=gateway_secret,
                                         auto_convert=True))
 
# Import the classes used by PySpark
java_import(gateway.jvm, "org.apache.spark.SparkConf")
java_import(gateway.jvm, "org.apache.spark.api.java.*")
java_import(gateway.jvm, "org.apache.spark.api.python.*")
java_import(gateway.jvm, "org.apache.spark.ml.python.*")
java_import(gateway.jvm, "org.apache.spark.mllib.api.python.*")
# TODO(davies): move into sql
java_import(gateway.jvm, "org.apache.spark.sql.*")
java_import(gateway.jvm, "org.apache.spark.sql.api.python.*")
java_import(gateway.jvm, "org.apache.spark.sql.hive.*")
java_import(gateway.jvm, "scala.Tuple2")
 
return gateway
```

这里面主要做了什么呢，简单说明一下就是这样配置了以后，python可以直接调用java的函数，`JavaGateway`是链接到JVM，`java_import`就是把相关的存在于JVM的包引入进来。关于java和python混合编程，后期会有单独的文章对`py4j`进行单独的讲解。可以先参考一下[py4j官方](https://www.py4j.org/py4j_java_gateway.html)的文档。

其中`gateway`是一个python的类`JavaGateway`的一个实例，它有一个属性`jvm`，可以简单理解为java的`jvm`，我们可以通过比如`gateway.jvm.java.util.ArrayList()`的方式在python里面创建一个java的`ArrayList`对象。

好，现在我们知道python里面的`SparkContext._gateway`和`SparkContext._jvm`代表什么意思了。因为这两个在后期经常会用到，所以一定要先理解是什么意思。

首先从环境变量中拿到环境变量PYSPARK_GATEWAY_PORT，这个就是我们在PythonRunner中设置的环境变量，然后启动一个JavaGateway同GatewayServer进行通讯，最后把Python Api中需要的Java|Scala的类引入引进来，完成了上面的工作后我们就可以真正的初始化SparkContext了
我们回到`SparkContext.__init__()`，分析如何`_jsc`变量的初始化（其中`_jsc`就是JVM中的SparkContext对象在Python中的影子。

#### 5 `_do_init`方法

在调用`_do_init`才是做了很多真正的初始化操作

```
    def _do_init(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer,
                 conf, jsc, profiler_cls):
        self.environment = environment or {}
        # java gateway must have been launched at this point.
        if conf is not None and conf._jconf is not None:
            # conf has been initialized in JVM properly, so use conf directly. This represents the
            # scenario that JVM has been launched before SparkConf is created (e.g. SparkContext is
            # created and then stopped, and we create a new SparkConf and new SparkContext again)
            self._conf = conf
        else:
            self._conf = SparkConf(_jvm=SparkContext._jvm)
            if conf is not None:
                for k, v in conf.getAll():
                    self._conf.set(k, v)

        self._batchSize = batchSize  # -1 represents an unlimited batch size
        self._unbatched_serializer = serializer
        if batchSize == 0:
            self.serializer = AutoBatchedSerializer(self._unbatched_serializer)
        else:
            self.serializer = BatchedSerializer(self._unbatched_serializer,
                                                batchSize)

        # Set any parameters passed directly to us on the conf
        if master:
            self._conf.setMaster(master)
        if appName:
            self._conf.setAppName(appName)
        if sparkHome:
            self._conf.setSparkHome(sparkHome)
        if environment:
            for key, value in environment.items():
                self._conf.setExecutorEnv(key, value)
        for key, value in DEFAULT_CONFIGS.items():
            self._conf.setIfMissing(key, value)

        # Check that we have at least the required parameters
        if not self._conf.contains("spark.master"):
            raise Exception("A master URL must be set in your configuration")
        if not self._conf.contains("spark.app.name"):
            raise Exception("An application name must be set in your configuration")

        # Read back our properties from the conf in case we loaded some of them from
        # the classpath or an external config file
        self.master = self._conf.get("spark.master")
        self.appName = self._conf.get("spark.app.name")
        self.sparkHome = self._conf.get("spark.home", None)

        for (k, v) in self._conf.getAll():
            if k.startswith("spark.executorEnv."):
                varName = k[len("spark.executorEnv."):]
                self.environment[varName] = v

        self.environment["PYTHONHASHSEED"] = os.environ.get("PYTHONHASHSEED", "0")

        # Create the Java SparkContext through Py4J
        self._jsc = jsc or self._initialize_context(self._conf._jconf)
        # Reset the SparkConf to the one actually used by the SparkContext in JVM.
        self._conf = SparkConf(_jconf=self._jsc.sc().conf())

        # Create a single Accumulator in Java that we'll send all our updates through;
        # they will be passed back to us through a TCP server
        auth_token = self._gateway.gateway_parameters.auth_token
        self._accumulatorServer = accumulators._start_update_server(auth_token)
        (host, port) = self._accumulatorServer.server_address
        self._javaAccumulator = self._jvm.PythonAccumulatorV2(host, port, auth_token)
        self._jsc.sc().register(self._javaAccumulator)

        # If encryption is enabled, we need to setup a server in the jvm to read broadcast
        # data via a socket.
        # scala's mangled names w/ $ in them require special treatment.
        self._encryption_enabled = self._jvm.PythonUtils.isEncryptionEnabled(self._jsc)

        self.pythonExec = os.environ.get("PYSPARK_PYTHON", 'python')
        self.pythonVer = "%d.%d" % sys.version_info[:2]

        if sys.version_info < (3, 6):
            with warnings.catch_warnings():
                warnings.simplefilter("once")
                warnings.warn(
                    "Support for Python 2 and Python 3 prior to version 3.6 is deprecated as "
                    "of Spark 3.0. See also the plan for dropping Python 2 support at "
                    "https://spark.apache.org/news/plan-for-dropping-python-2-support.html.",
                    DeprecationWarning)

        # Broadcast's __reduce__ method stores Broadcast instances here.
        # This allows other code to determine which Broadcast instances have
        # been pickled, so it can determine which Java broadcast objects to
        # send.
        self._pickled_broadcast_vars = BroadcastPickleRegistry()

        SparkFiles._sc = self
        root_dir = SparkFiles.getRootDirectory()
        sys.path.insert(1, root_dir)

        # Deploy any code dependencies specified in the constructor
        self._python_includes = list()
        for path in (pyFiles or []):
            self.addPyFile(path)

        # Deploy code dependencies set by spark-submit; these will already have been added
        # with SparkContext.addFile, so we just need to add them to the PYTHONPATH
        for path in self._conf.get("spark.submit.pyFiles", "").split(","):
            if path != "":
                (dirname, filename) = os.path.split(path)
                try:
                    filepath = os.path.join(SparkFiles.getRootDirectory(), filename)
                    if not os.path.exists(filepath):
                        # In case of YARN with shell mode, 'spark.submit.pyFiles' files are
                        # not added via SparkContext.addFile. Here we check if the file exists,
                        # try to copy and then add it to the path. See SPARK-21945.
                        shutil.copyfile(path, filepath)
                    if filename[-4:].lower() in self.PACKAGE_EXTENSIONS:
                        self._python_includes.append(filename)
                        sys.path.insert(1, filepath)
                except Exception:
                    warnings.warn(
                        "Failed to add file [%s] speficied in 'spark.submit.pyFiles' to "
                        "Python path:\n  %s" % (path, "\n  ".join(sys.path)),
                        RuntimeWarning)

        # Create a temporary directory inside spark.local.dir:
        local_dir = self._jvm.org.apache.spark.util.Utils.getLocalDir(self._jsc.sc().conf())
        self._temp_dir = \
            self._jvm.org.apache.spark.util.Utils.createTempDir(local_dir, "pyspark") \
                .getAbsolutePath()

        # profiling stats collected for each PythonRDD
        if self._conf.get("spark.python.profile", "false") == "true":
            dump_path = self._conf.get("spark.python.profile.dump", None)
            self.profiler_collector = ProfilerCollector(profiler_cls, dump_path)
        else:
            self.profiler_collector = None

        # create a signal handler which would be invoked on receiving SIGINT
        def signal_handler(signal, frame):
            self.cancelAllJobs()
            raise KeyboardInterrupt()

        # see http://stackoverflow.com/questions/23206787/
        if isinstance(threading.current_thread(), threading._MainThread):
            signal.signal(signal.SIGINT, signal_handler)
```

1. 其实前面很大一段逻辑都很简单，就是调用了`SparkConf`对象来设置一些属性，并且获取一些属性
2. 在接着就是获取和设置一些环境变量，还有jvm相关的内容
3. 里面使用了`self._conf = SparkConf(_jvm=SparkContext._jvm)`代码，那么`SparkConf`对象，也就获得了jvm这个实例，通过调用`SparkConf`实例的`set`函数进行属性配置，这个配置，实际上就是调用了java的`SparkConf`对象的`set`函数。相当于已经实实在在的配置到jvm环境里面了。

#### 5 后续执行

完成了SparkContext的初始化后，就能在业务代码实现自己的逻辑了，如示例`pi.py`中使用：

```
```

### 3、Python Driver 端的 RDD、SQL 接口

在 PySpark 中，继续初始化一些 Python 和 JVM 的环境后，Python 端的 SparkContext 对象就创建好了，它实际是对 JVM 端接口的一层封装。和 Scala API 类似，SparkContext 对象也提供了各类创建 RDD 的接口，和 Scala API 基本一一对应，我们来看一些例子。

#### 1、newAPIHadoopFile

```
def newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None,
                         valueConverter=None, conf=None, batchSize=0):
     jconf = self._dictToJavaMap(conf)
     jrdd = self._jvm.PythonRDD.newAPIHadoopFile(self._jsc, path, inputFormatClass, keyClass,
                                                    valueClass, keyConverter, valueConverter,
                                                    jconf, batchSize)
     return RDD(jrdd, self)   
```

可以看到，这里 Python 端基本就是直接调用了 Java/Scala接口。而 PythonRDD (core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala)，则是一个 Scala 中封装的伴生对象，提供了常用的 RDD IO 相关的接口。

```
private[spark] class PythonRDD(
    parent: RDD[_],
    func: PythonFunction,
    preservePartitoning: Boolean,
    isFromBarrier: Boolean = false)
  extends RDD[Array[Byte]](parent) {

  override def getPartitions: Array[Partition] = firstParent.partitions

  override val partitioner: Option[Partitioner] = {
    if (preservePartitoning) firstParent.partitioner else None
  }

  val asJavaRDD: JavaRDD[Array[Byte]] = JavaRDD.fromRDD(this)

  override def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]] = {
    val runner = PythonRunner(func)
    runner.compute(firstParent.iterator(split, context), split.index, context)
  }

  @transient protected lazy override val isBarrier_ : Boolean =
    isFromBarrier || dependencies.exists(_.rdd.isBarrier())
}
```

PythonRDD类的newAPIHadoopFile方法

```

  /**
   * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]],
   * key and value class.
   * A key and/or value converter class can optionally be passed in
   * (see [[org.apache.spark.api.python.Converter]])
   */
  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](
      sc: JavaSparkContext,
      path: String,
      inputFormatClass: String,
      keyClass: String,
      valueClass: String,
      keyConverterClass: String,
      valueConverterClass: String,
      confAsMap: java.util.HashMap[String, String],
      batchSize: Int): JavaRDD[Array[Byte]] = {
    val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration())
    val rdd =
      newAPIHadoopRDDFromClassNames[K, V, F](sc,
        Some(path), inputFormatClass, keyClass, valueClass, mergedConf)
    val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf))
    val converted = convertRDD(rdd, keyConverterClass, valueConverterClass,
      new WritableToJavaConverter(confBroadcasted))
    JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize))
  }
```

另外一些接口会通过 `self._jsc` 对象去创建 RDD。其中 `self._jsc` 就是 JVM 中的 SparkContext 对象。拿到 RDD 对象之后，可以像 Scala、Java API 一样，对 RDD 进行各类操作，这些大部分都封装在 python/pyspark/rdd.py 中。

这里的代码中出现了 jrdd 这样一个对象，这实际上是 Scala 为提供 Java 互操作的 RDD 的一个封装，用来提供 Java 的 RDD 接口，具体实现在 core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala 中。可以看到每个 Python 的 RDD 对象需要用一个 JavaRDD 对象去创建。

对于 DataFrame 接口，Python 层也同样提供了 SparkSession、DataFrame 对象，它们也都是对 Java 层接口的封装，这里不一一赘述。

### 4、Executor 端进程间通信和序列化

对于 Spark 内置的算子，在 Python 中调用 RDD、DataFrame 的接口后，从上文可以看出会通过 JVM 去调用到 Scala 的接口，最后执行和直接使用 Scala 并无区别。而对于需要使用 UDF 的情形，在 Executor 端就需要启动一个 Python worker 子进程，然后执行 UDF 的逻辑。那么 Spark 是怎样判断需要启动子进程的呢？

在 Spark 编译用户的 DAG 的时候，Catalyst Optimizer 会创建 BatchEvalPython 或者 ArrowEvalPython 这样的 Logical Operator，随后会被转换成 PythonEvals 这个 Physical Operator。在 PythonEvals（sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala）中：

```
  /**
   * Strategy to convert EvalPython logical operator to physical operator.
   */
  object PythonEvals extends Strategy {
    override def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
      case ArrowEvalPython(udfs, output, child) =>
        ArrowEvalPythonExec(udfs, output, planLater(child)) :: Nil
      case BatchEvalPython(udfs, output, child) =>
        BatchEvalPythonExec(udfs, output, planLater(child)) :: Nil
      case _ =>
        Nil
    }
  }
```

创建了 ArrowEvalPythonExec 或者 BatchEvalPythonExec，而这二者内部会创建 ArrowPythonRunner、PythonUDFRunner 等类的对象实例，并调用了它们的 compute 方法。由于它们都继承了 BasePythonRunner，基类的 compute 方法中会去启动 Python 子进程：

```
def compute():
	  val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)
	  // Start a thread to feed the process input from our parent's iterator
    val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context)
     writerThread.start()
     
      // Return an iterator that read lines from the process's stdout
    val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))
    val stdoutIterator = newReaderIterator(
      stream, writerThread, startTime, env, worker, releasedOrClosed, context)
    new InterruptibleIterator(context, stdoutIterator)
```

这里 env.createPythonWorker 会通过 PythonWorkerFactory （core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala）去启动 Python 进程。Executor 端启动 Python 子进程后，会创建一个 socket 与 Python 建立连接。所有 RDD 的数据都要序列化后，通过 socket 发送，而结果数据需要同样的方式序列化传回 JVM。

对于直接使用 RDD 的计算，或者没有开启 spark.sql.execution.arrow.enabled 的 DataFrame，是将输入数据按行发送给 Python，可想而知，这样效率极低。

在 Spark 2.2 后提供了基于 Arrow 的序列化、反序列化的机制（从 3.0 起是默认开启），从 JVM 发送数据到 Python 进程的代码在 sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala。这个类主要是重写了 newWriterThread 这个方法，使用了 ArrowWriter 向 socket 发送数据：

```
     val arrowWriter = ArrowWriter.create(root)
          val writer = new ArrowStreamWriter(root, null, dataOut)
          writer.start()

          while (inputIterator.hasNext) {
            val nextBatch = inputIterator.next()

            while (nextBatch.hasNext) {
              arrowWriter.write(nextBatch.next())
            }

            arrowWriter.finish()
            writer.writeBatch()
            arrowWriter.reset()
```

可以看到， 每次取出一个batch，填充给 ArrowWriter，实际数据会保存在 root 对象中，然后由 ArrowStreamWriter 将 root 对象中的整个 batch 的数据写入到 socket 的 DataOutputStream 中去。ArrowStreamWriter 会调用 writeBatch 方法去序列化消息并写数据，代码参考 ArrowWriter.java#L131。

```
protected ArrowBlock writeRecordBatch(ArrowRecordBatch batch) throws IOException {
 ArrowBlock block = MessageSerializer.serialize(out, batch, option);
 LOGGER.debug("RecordBatch at {}, metadata: {}, div: {}",
     block.getOffset(), block.getMetadataLength(), block.getBodyLength());
 return block;
}
```

在 MessageSerializer 中，使用了 flatbuffer 来序列化数据。flatbuffer 是一种比较高效的序列化协议，它的主要优点是反序列化的时候，不需要解码，可以直接通过裸 buffer 来读取字段，可以认为反序列化的开销为零。我们来看看 Python 进程收到消息后是如何反序列化的。

Python 子进程实际上是执行了 worker.py 的 main 函数 (python/pyspark/worker.py)：

```
if __name__ == '__main__':
    # Read information about how to connect back to the JVM from the environment.
    java_port = int(os.environ["PYTHON_WORKER_FACTORY_PORT"])
    auth_secret = os.environ["PYTHON_WORKER_FACTORY_SECRET"]
    (sock_file, _) = local_connect_and_auth(java_port, auth_secret)
    main(sock_file, sock_file)
```

这里会去向 JVM 建立连接，并从 socket 中读取指令和数据。对于如何进行序列化、反序列化，是通过 UDF 的类型来区分：

```
   split_index = read_int(infile)
   if eval_type == PythonEvalType.NON_UDF:
   func, profiler, deserializer, serializer = read_command(pickleSer, infile)
else:
   func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)
```

在 read_udfs 中，如果是 PANDAS 类的 UDF，会创建 ArrowStreamPandasUDFSerializer，其余的 UDF 类型创建 BatchedSerializer。

```
 ser = ArrowStreamPandasUDFSerializer(timezone, safecheck, assign_cols_by_name,
                                                 df_for_struct)
```

我们来看看 ArrowStreamPandasUDFSerializer真正执行的方法(python/pyspark/serializers.py)：

```
class ArrowStreamSerializer(Serializer):
    """
    Serializes Arrow record batches as a stream.
    """

    def dump_stream(self, iterator, stream):
        import pyarrow as pa
        writer = None
        try:
            for batch in iterator:
                if writer is None:
                    writer = pa.RecordBatchStreamWriter(stream, batch.schema)
                writer.write_batch(batch)
        finally:
            if writer is not None:
                writer.close()
                
     def load_stream(self, stream):
        import pyarrow as pa
        reader = pa.ipc.open_stream(stream)
        for batch in reader:
            yield batch            
```

可以看到，这里双向的序列化、反序列化，都是调用了 PyArrow 的 ipc 的方法，和前面看到的 Scala 端是正好对应的，也是按 batch 来读写数据。对于 Pandas 的 UDF，读到一个 batch 后，会将 Arrow 的 batch 转换成 Pandas Series。

```
    def arrow_to_pandas(self, arrow_column):
        from pyspark.sql.pandas.types import _check_series_localize_timestamps
        import pyarrow

        # If the given column is a date type column, creates a series of datetime.date directly
        # instead of creating datetime64[ns] as intermediate data to avoid overflow caused by
        # datetime64[ns] type handling.
        s = arrow_column.to_pandas(date_as_object=True)

        if pyarrow.types.is_timestamp(arrow_column.type):
            return _check_series_localize_timestamps(s, self._timezone)
        else:
            return s
```

### 5、**Pandas UDF**

前面我们已经看到，PySpark 提供了基于 Arrow 的进程间通信来提高效率，那么对于用户在 Python 层的 UDF，是不是也能直接使用到这种高效的内存格式呢？答案是肯定的，这就是 PySpark 推出的 Pandas UDF。区别于以往以行为单位的 UDF，Pandas UDF 是以一个 Pandas Series 为单位，batch 的大小可以由 spark.sql.execution.arrow.maxRecordsPerBatch 这个参数来控制。这是一个来自官方文档的示例：

```
def multiply_func(a, b):
   return a * b

multiply = pandas_udf(multiply_func, returnType=LongType())
df.select(multiply(col("x"), col("x"))).show()
```

上文已经解析过，PySpark 会将 DataFrame 以 Arrow 的方式传递给 Python 进程，Python 中会转换为 Pandas Series，传递给用户的 UDF。在 Pandas UDF 中，可以使用 Pandas 的 API 来完成计算，在易用性和性能上都得到了很大的提升。

## 六、总结

PySpark 为用户提供了 Python 层对 RDD、DataFrame 的操作接口，同时也支持了 UDF，通过 Arrow、Pandas 向量化的执行，对提升大规模数据处理的吞吐是非常重要的，一方面可以让数据以向量的形式进行计算，提升 cache 命中率，降低函数调用的开销，另一方面对于一些 IO 的操作，也可以降低网络延迟对性能的影响。

然而 PySpark 仍然存在着一些不足，主要有：

1、进程间通信消耗额外的 CPU 资源；

2、编程接口仍然需要理解 Spark 的分布式计算原理；

3、Pandas UDF 对返回值有一定的限制，返回多列数据不太方便；

 Vectorized Execution 的推进，有望在 Spark 内部一切数据都是用 Arrow 的格式来存放，对跨语言支持将会更加友好。