## 一 SparkContext上下文

![](image\SparkContext.png)

## 二 RDD依赖关系

```
val fileRDD: RDD[String] = sc.textFile("D:\\bigdata\\spark\\spark-demo\\input\\word.txt")
// 2、每一行数据进行拆分，形成一个一个单词
val words = fileRDD.flatMap(_.split(" "))
// 3. 将数据进行分组，便于统计 (word,iterator)
val wordGroup = words.groupBy(word=>word)
// 4 分组后的数据进行转换

val wordToCount = wordGroup.map{
case(word,list) =>{
(word,list.size)
}
}
// 5 打印
val arr = wordToCount.collect()
```



![](image\RDD依赖.png)

### 1、起点是一个MapPartitionsRDD

```
 def map[U: ClassTag](f: T => U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF))
  }
```

经过flatMap之后，产生一个新的MapPartitionsRDD。这个新的RDD包含了之前的MapPartitionsRDD，即产生了依赖。

这里是OneToOne依赖

```
def this(@transient oneParent: RDD[_]) =
    this(oneParent.context, List(new OneToOneDependency(oneParent)))
```

最终的依赖关系可以通过`getDependencies`方法得到：

```
 protected def getDependencies: Seq[Dependency[_]] = deps
```

### 2、ShuffledRDD

经过`groupByKey`之后，产生一个ShuffledRDD

```
 new ShuffledRDD[K, V, V](self, partitioner)
```

而对应的依赖就是`ShuffleDependency`

```
 List(new ShuffleDependency(prev, part, serializer, keyOrdering, aggregator, mapSideCombine))
```

## 三、阶段的划分

SparkContext中的 runJob方法里面，会执行`dagScheduler.runJob`

```
dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
 // 然后runJob里面执行submitJob
val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
// 会将其提交到一个事件队列中
 eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter,
      Utils.cloneProperties(properties)))
```

事件放到事件队列之后，`eventThread`线程会将事件从队列中循环取出

```
private[spark] val eventThread = new Thread(name) {
    setDaemon(true)

    override def run(): Unit = {
      try {
        while (!stopped.get) {
          val event = eventQueue.take()
           onReceive(event)==>doOnReceive(event)==>dagScheduler.handleJobSubmitted(...)
```

<span name = "stage">stage</span>

然后开始阶段的划分

```
 finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
```

在`createResultStage`里面：

### 1、首先获取或者创建上级阶段：

```
  val parents = getOrCreateParentStages(rdd, jobId)
```

 ==> `getOrCreateParentStages`核心代码，获取shuffle依赖。然后对shuffle依赖进行处理

获取或者创建ShuffleMap阶段。即shuffle之前的阶段，其要写磁盘。

```
getShuffleDependencies(rdd).map { shuffleDep =>
      getOrCreateShuffleMapStage(shuffleDep, firstJobId)
    }.toList
```

` getOrCreateShuffleMapStage`核心代码：

```
 createShuffleMapStage(shuffleDep, firstJobId)
 ==> shuffle依赖的rdd就是前面的最后的那个紫色rdd,把这个rdd传过来
 	  val rdd = shuffleDep.rdd
 	  val stage = new ShuffleMapStage(
      id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep, mapOutputTracker)
```

在`ShuffleMapStage`里面，也会进行判断 紫色rdd 是否有上级阶段，如果有会递归执行上面代码。

```
 val parents = getOrCreateParentStages(rdd, jobId)
```



### 2、创建ResultStage

```
==> val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)
```

### 3、流程示意图

![](image\阶段划分.png)



从最后的rdd开始反向传播，碰到shuffle就创建一个ShuffleMapStage。然后最后的rdd本身创建一个ResultStage，所有最终的stage=shuffle数+1。



## 四、任务的切分

一个阶段执行完成之后，才会执行下一个阶段。

回到handleJobSubmitted方法，现在阶段划分完成，<a href="#stage">前面的代码</a>

继续执行后续代码

```
 submitStage(finalStage){
     // 从黄色rdd开始获取还没有被提交的上一级
 	 val missing = getMissingParentStages(stage).sortBy(_.id)
 	 if (missing.isEmpty) { // 如果是空，说明是没有parent的stage，或者上一级stage已经被提交；则提交它自己。
          logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
          submitMissingTasks(stage, jobId.get)
      // 然后对于有上一级的，递归执行submitStage。提交每个stage    
 	  for (parent <- missing) {
            submitStage(parent)
          }
 }
```

如果没有上一级开始执行`submitMissingTasks`，其核心代码为：对stage进行判断，如果是shuffle 阶段，一个分区创建一个task。

如果是最后的result 阶段，就创建ResultTask

```
stage match {
        case stage: ShuffleMapStage =>
          stage.pendingPartitions.clear()
          partitionsToCompute.map { id =>
            val locs = taskIdToLocations(id)
            val part = partitions(id)
            stage.pendingPartitions += id
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
          }

        case stage: ResultStage =>
          partitionsToCompute.map { id =>
            val p: Int = stage.partitions(id)
            val part = partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, id, properties, serializedTaskMetrics,
              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
              stage.rdd.isBarrier())
          }
      }
```

`partitionsToCompute`的值为：如果有5个分区，=[0,1,2,3,4]

```
  /** Returns the sequence of partition ids that are missing (i.e. needs to be computed). */
  override def findMissingPartitions(): Seq[Int] = {
    mapOutputTrackerMaster
      .findMissingPartitions(shuffleDep.shuffleId)
      .getOrElse(0 until numPartitions)
  }
}
```

有几个id就创建几个task对象。

task的总数量为每个stage中最后一个RDD分区的个数累加。

## 五、任务的调度

如何把task发送到某个executor去执行？

tasks已经切分完毕。执行`new TaskSet`，task会包装成TaskSet，再次封装为TaskSet Manager。然后调度器提交任务。

```
if (tasks.nonEmpty) {
      logInfo(s"Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 " +
        s"tasks are for partitions ${tasks.take(15).map(_.partitionId)})")
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))
```

### 5.1 submitTasks

首先封装`TaskSet Manager`，得到任务集管理器

```
 val manager = createTaskSetManager(taskSet, maxTaskFailures)
```

然后把任务集管理器放到调度器里

```
 schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
```

其中schedulableBuilder =  new FIFOSchedulableBuilder(rootPool)，默认为FIFO调度器

```
  def initialize(backend: SchedulerBackend): Unit = {
    this.backend = backend
    schedulableBuilder = {
      schedulingMode match {
        case SchedulingMode.FIFO =>
          new FIFOSchedulableBuilder(rootPool)
        case SchedulingMode.FAIR =>
          new FairSchedulableBuilder(rootPool, conf)
        case _ =>
          throw new IllegalArgumentException(s"Unsupported $SCHEDULER_MODE_PROPERTY: " +
          s"$schedulingMode")
      }
    }
    schedulableBuilder.buildPools()
  }
```

### 5.2 addTaskSetManager

把manager放到任务池中

```
override def addTaskSetManager(manager: Schedulable, properties: Properties): Unit = {
    rootPool.addSchedulable(manager)
  }
```

![](image\任务池.png)

### 5.3 backend.reviveOffers()

前面任务放到了任务池中，接下来从任务池中取任务

`CoarseGrainedSchedulerBackend`类执行下面方法

```
 override def reviveOffers(): Unit = {
    driverEndpoint.send(ReviveOffers)
  }
```

自己给自己发消息，然后收到这个消息之后执行`makeOffers()`

```
  case ReviveOffers =>
        makeOffers()
```

在这里：1、得到任务描述信息；2、如果任务不为空，启动任务

```
private def makeOffers(): Unit = {
      // Make sure no executor is killed while some task is launching on it
      val taskDescs = withLock {
        // Filter out executors under killing
        val activeExecutors = executorDataMap.filterKeys(isExecutorActive)
        val workOffers = activeExecutors.map {
          case (id, executorData) =>
            new WorkerOffer(id, executorData.executorHost, executorData.freeCores,
              Some(executorData.executorAddress.hostPort),
              executorData.resourcesInfo.map { case (rName, rInfo) =>
                (rName, rInfo.availableAddrs.toBuffer)
              })
        }.toIndexedSeq
        scheduler.resourceOffers(workOffers)
      }
      if (taskDescs.nonEmpty) {
        launchTasks(taskDescs)
      }
    }
```

而任务从任务池取出来是通过：

Called by cluster manager to offer resources on slaves. We respond by asking our active task sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so that tasks are balanced across the cluster.

```
 scheduler.resourceOffers(workOffers)
```

拿到排序完成的任务集

```
val sortedTaskSets = rootPool.getSortedTaskSetQueue.filterNot(_.isZombie)
```

遍历，然后进行本地化级别操作

```
  for (taskSet <- sortedTaskSets) {
  	for (currentMaxLocality <- taskSet.myLocalityLevels) {
  	
```





本地化级别。task是一个计算。task最好发给数据所在的节点。如果是其它节点，数据要拉取过来。移动数据不如移动计算。

<img src="image\本地化级别.png" style="zoom:80%;" />

四种级别：

1、进程本地化，数据和计算在同一个内存中。即同一个进程中

2、节点本地化，数据和计算同一个节点。即同一个机器

3、机架本地化，同一个机架。

4、任意放

最后返回tasks

```
return tasks
```

### 5.4 launchTasks

得到返回的任务集，然后遍历任务集中的任务，对任务进行编码，即序列化。

找到对应的executer终端，向它发送消息。消息本体为启动序列化的任务。

每个任务发送到哪个executor，根据前面的本地化级别选出。

```
for (task <- tasks.flatten) {
	  val serializedTask = TaskDescription.encode(task)
	 executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
```

## 六、任务的执行

![](image\任务的执行.png)

在`CoarseGrainedExecutorBackend`类中，会接收到 5.4发送的`LaunchTask`

1、当前任务反序列化，即解码。

2、executor执行任务

```
 case LaunchTask(data) =>
      if (executor == null) {
        exitExecutor(1, "Received LaunchTask command but executor was null")
      } else {
        val taskDesc = TaskDescription.decode(data.value)
        logInfo("Got assigned task " + taskDesc.taskId)
        taskResources(taskDesc.taskId) = taskDesc.resources
        executor.launchTask(this, taskDesc)
      }
```

### 6.2 executor.launchTask

```
  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
    val tr = new TaskRunner(context, taskDescription)
    runningTasks.put(taskDescription.taskId, tr)
    threadPool.execute(tr)
  }
```

executor有一个线程池，来一个task，有一个线程去运行这个task。

### 6.3 run()

一路寻找run,最后在一个抽象方法

```
 def runTask(context: TaskContext): T
```

executor去运行task，但是每个task要完成什么rdd的操作，是task它自己决定的。